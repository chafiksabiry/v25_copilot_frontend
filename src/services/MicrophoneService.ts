export class MicrophoneService {
  private outboundWs: WebSocket | null = null;
  private audioContext: AudioContext | null = null;
  private node: AudioWorkletNode | null = null;
  private stream: MediaStream | null = null;
  private rawAudioBuffer: Float32Array[] = [];
  private recordingStartTime: number = 0;
  private recordingInterval: number | null = null;
  private recorderScriptNode: ScriptProcessorNode | null = null;
  private recorderWorkletNode: AudioWorkletNode | null = null; // Nouveau worklet pour enregistrement
  private recordingCounter: number = 0;
  private isRecording: boolean = false;
  private hasStartedCapture: boolean = false; // Flag pour √©viter la double capture

  constructor(outboundWs: WebSocket) {
    this.outboundWs = outboundWs;
  }

  // Static method to test microphone permissions before starting capture
  static async testMicrophonePermissions(): Promise<{ success: boolean; error?: string }> {
    try {
      console.log('üß™ Testing microphone permissions...');
      
      // Check if we're in a secure context
      if (!window.isSecureContext && location.protocol !== 'https:' && location.hostname !== 'localhost') {
        return {
          success: false,
          error: 'Microphone access requires HTTPS or localhost. Please use HTTPS or test on localhost.'
        };
      }

      // Check permissions API
      if (navigator.permissions) {
        const permissionStatus = await navigator.permissions.query({ name: 'microphone' as PermissionName });
        console.log('üé§ Permission status:', permissionStatus.state);
        
        if (permissionStatus.state === 'denied') {
          return {
            success: false,
            error: 'Microphone permission denied. Please allow microphone access in your browser settings.'
          };
        }
      }

      // Test actual microphone access
      const stream = await navigator.mediaDevices.getUserMedia({ 
        audio: {
          echoCancellation: true,
          noiseSuppression: true,
          autoGainControl: true
        } 
      });
      
      // Clean up test stream
      stream.getTracks().forEach(track => track.stop());
      
      console.log('‚úÖ Microphone permissions test passed');
      return { success: true };
      
    } catch (error: any) {
      console.error('‚ùå Microphone permissions test failed:', error);
      
      let errorMessage = 'Unknown microphone error';
      if (error.name === 'NotAllowedError') {
        errorMessage = 'Microphone permission denied. Please click "Allow" when prompted.';
      } else if (error.name === 'NotFoundError') {
        errorMessage = 'No microphone found. Please connect a microphone.';
      } else if (error.name === 'NotReadableError') {
        errorMessage = 'Microphone is being used by another application.';
      } else if (error.name === 'OverconstrainedError') {
        errorMessage = 'Microphone constraints cannot be satisfied.';
      }
      
      return { success: false, error: errorMessage };
    }
  }

  async startCapture() {
    // Emp√™cher la double capture
    if (this.hasStartedCapture) {
      console.warn('‚ö†Ô∏è Capture d√©j√† d√©marr√©e, ignor√©');
      return;
    }
    
    try {
      this.hasStartedCapture = true;
      
      // 1) Ensure outbound WebSocket provided and open
      if (!this.outboundWs) throw new Error('Outbound WebSocket instance not provided');
      if (this.outboundWs.readyState !== WebSocket.OPEN) {
        await new Promise<void>((resolve, reject) => {
          const onOpen = () => { this.outboundWs?.removeEventListener('open', onOpen as any); resolve(); };
          const onError = () => { this.outboundWs?.removeEventListener('error', onError as any); reject(new Error('Outbound WebSocket error')); };
          this.outboundWs?.addEventListener('open', onOpen as any);
          this.outboundWs?.addEventListener('error', onError as any);
        });
      }

      // 2) Check microphone permissions first
      console.log('üé§ Checking microphone permissions...');
      const permissionStatus = await navigator.permissions.query({ name: 'microphone' as PermissionName });
      console.log('üé§ Microphone permission status:', permissionStatus.state);
      
      if (permissionStatus.state === 'denied') {
        throw new Error('Microphone permission denied. Please allow microphone access in your browser settings.');
      }

      // 3) Capture microphone with optimized audio constraints for call quality
      console.log('üé§ Requesting microphone access...');
      try {
        // Configuration optimale pour r√©duire les bruits automatiquement
        // IMPORTANT: Essayer de forcer la capture √† 8kHz pour correspondre au codec PCMA/PCMU
        // Cela r√©duit les artefacts de resampling et les bruits
        this.stream = await navigator.mediaDevices.getUserMedia({ 
          audio: {
            // Traitement audio natif du navigateur (priorit√© haute)
            echoCancellation: true,        // Annulation d'√©cho pour √©viter le feedback
            noiseSuppression: true,        // Suppression de bruit de fond
            autoGainControl: true,         // Contr√¥le automatique du gain (√©vite saturation)
            
            // Param√®tres avanc√©s pour meilleure qualit√©
            // Essayer 8kHz d'abord pour correspondre au codec PCMA/PCMU
            sampleRate: 8000,             // Taux d'√©chantillonnage correspondant au codec (8kHz)
            channelCount: 1,              // Mono (suffisant pour la voix)
            latency: 0.01,                // Latence minimale (10ms)
            
            // Contraintes pour forcer l'activation des fonctionnalit√©s
            googEchoCancellation: true,   // Google-specific (Chrome)
            googNoiseSuppression: true,   // Google-specific (Chrome)
            googAutoGainControl: true,    // Google-specific (Chrome)
            googHighpassFilter: true,     // Filtre passe-haut pour r√©duire basses fr√©quences
            googTypingNoiseDetection: true, // D√©tection bruit de frappe clavier
            
            // Param√®tres de qualit√©
            volume: 1.0,                  // Volume maximum (le navigateur ajustera automatiquement)
            suppressLocalAudioPlayback: false // Permettre la lecture locale si n√©cessaire
          } 
        });
        console.log('‚úÖ Microphone access granted with optimized audio settings');
        
        // V√©rifier les contraintes appliqu√©es (pour debug)
        const audioTracks = this.stream.getAudioTracks();
        if (audioTracks.length > 0) {
          const settings = audioTracks[0].getSettings();
          const actualSampleRate = settings.sampleRate || 48000; // Fallback si non disponible
          const requestedSampleRate = 8000;
          
          console.log('üé§ Applied audio settings:', {
            echoCancellation: settings.echoCancellation,
            noiseSuppression: settings.noiseSuppression,
            autoGainControl: settings.autoGainControl,
            sampleRate: actualSampleRate,
            channelCount: settings.channelCount
          });
          
          // Avertir si le sample rate r√©el ne correspond pas √† la demande
          if (actualSampleRate && Math.abs(actualSampleRate - requestedSampleRate) > 100) {
            console.warn(`‚ö†Ô∏è Microphone sample rate is ${actualSampleRate}Hz instead of ${requestedSampleRate}Hz`);
            console.warn('üí° Le navigateur a ignor√© la contrainte sampleRate. Le resampling sera effectu√© dans le worklet.');
          } else if (actualSampleRate) {
            console.log(`‚úÖ Microphone sample rate matches codec: ${actualSampleRate}Hz`);
          }
        }
      } catch (mediaError: any) {
        console.error('‚ùå Microphone access error:', mediaError);
        if (mediaError.name === 'NotAllowedError') {
          throw new Error('Microphone permission denied. Please click "Allow" when prompted or check your browser settings.');
        } else if (mediaError.name === 'NotFoundError') {
          throw new Error('No microphone found. Please connect a microphone and try again.');
        } else if (mediaError.name === 'NotReadableError') {
          throw new Error('Microphone is being used by another application. Please close other applications and try again.');
        } else {
          throw new Error(`Microphone error: ${mediaError.message}`);
        }
      }

      // 4) Create AudioContext with optimized settings for call quality
      // IMPORTANT: Forcer le sampleRate √† 8000Hz ou 48000Hz pour √©viter les ratios non entiers
      // qui causent une d√©rive d'horloge RTP. Le worklet fera le resampling optimis√© avec un filtre FIR anti-aliasing.
      const audioTracks = this.stream.getAudioTracks();
      const microphoneSampleRate = audioTracks[0]?.getSettings()?.sampleRate || 48000;
      
      // Ordre de priorit√© pour le sampleRate :
      // 1. 8000Hz (id√©al, pas de resampling n√©cessaire)
      // 2. 48000Hz (ratio entier 6:1, resampling optimal)
      // 3. SampleRate du micro (si 8kHz ou 48kHz ne sont pas support√©s)
      const preferredRates = [8000, 48000];
      let selectedRate: number | null = null;
      
      // V√©rifier si le micro est d√©j√† √† 8kHz ou 48kHz
      if (Math.abs(microphoneSampleRate - 8000) < 100) {
        selectedRate = 8000;
      } else if (Math.abs(microphoneSampleRate - 48000) < 100) {
        selectedRate = 48000;
      } else {
        // Essayer les taux pr√©f√©r√©s dans l'ordre
        for (const rate of preferredRates) {
          try {
            const testContext = new (window.AudioContext || (window as any).webkitAudioContext)({
              sampleRate: rate,
              latencyHint: 'interactive'
            });
            const actualRate = testContext.sampleRate;
            testContext.close();
            
            if (Math.abs(actualRate - rate) < 100) {
              selectedRate = rate;
              break;
            }
          } catch (e) {
            // Continuer avec le taux suivant
            continue;
          }
        }
      }
      
      try {
        // Cr√©er l'AudioContext avec le taux s√©lectionn√© ou le taux du micro
        const targetRate = selectedRate || microphoneSampleRate;
        this.audioContext = new (window.AudioContext || (window as any).webkitAudioContext)({
          sampleRate: targetRate,
          latencyHint: 'interactive' // Latence minimale pour appels en temps r√©el
        });
        
        const actualSampleRate = this.audioContext.sampleRate;
        const ratio = actualSampleRate / 8000;
        const isIntegerRatio = Math.abs(ratio - Math.round(ratio)) < 0.001;
        
        console.log(`üîä AudioContext cr√©√© √† ${actualSampleRate}Hz (micro: ${microphoneSampleRate}Hz)`);
        
        if (actualSampleRate === 8000) {
          console.log(`‚úÖ AudioContext √† 8kHz - Pas de resampling n√©cessaire (ratio: 1)`);
        } else if (isIntegerRatio) {
          console.log(`‚úÖ AudioContext √† ${actualSampleRate}Hz - Ratio entier (${ratio.toFixed(0)}:1) pour resampling optimal`);
        } else {
          console.warn(`‚ö†Ô∏è AudioContext √† ${actualSampleRate}Hz - Ratio non entier (${ratio.toFixed(4)}:1)`);
          console.warn(`üí° Le worklet utilisera le resampling fractionnaire pour √©viter la d√©rive d'horloge RTP`);
          console.warn(`üí° Recommandation: Le navigateur devrait supporter 8kHz ou 48kHz pour un ratio entier`);
        }
        
        if (Math.abs(actualSampleRate - microphoneSampleRate) > 100) {
          console.log(`üí° Le navigateur fera un resampling automatique du micro (${microphoneSampleRate}Hz ‚Üí ${actualSampleRate}Hz)`);
          console.log(`üí° Le worklet effectuera ensuite le resampling optimis√© vers 8kHz avec filtre FIR anti-aliasing`);
        }
      } catch (error) {
        // Fallback : cr√©er avec le sample rate par d√©faut du navigateur
        console.warn('‚ö†Ô∏è Impossible de cr√©er AudioContext avec taux pr√©f√©r√©, utilisation du sample rate par d√©faut:', error);
        this.audioContext = new (window.AudioContext || (window as any).webkitAudioContext)({
          latencyHint: 'interactive'
        });
        const actualRate = this.audioContext.sampleRate;
        const ratio = actualRate / 8000;
        const isIntegerRatio = Math.abs(ratio - Math.round(ratio)) < 0.001;
        
        console.log(`üîä AudioContext cr√©√© √† ${actualRate}Hz (sample rate par d√©faut)`);
        if (isIntegerRatio) {
          console.log(`‚úÖ Ratio entier (${ratio.toFixed(0)}:1) pour resampling optimal`);
        } else {
          console.warn(`‚ö†Ô∏è Ratio non entier (${ratio.toFixed(4)}:1) - Le worklet utilisera le resampling fractionnaire`);
        }
      }
      
      // S'assurer que l'AudioContext est actif
      if (this.audioContext.state === 'suspended') {
        await this.audioContext.resume();
        console.log('üîä AudioContext resumed');
      }
      
      const source = this.audioContext.createMediaStreamSource(this.stream);
      
      // Cr√©er un filtre passe-bas suppl√©mentaire pour r√©duire les bruits haute fr√©quence
      // (le navigateur fait d√©j√† du noise suppression, mais on peut am√©liorer)
      // IMPORTANT: Ce filtre est compl√©mentaire au filtre FIR dans le worklet
      // Le filtre Biquad ici pr√©-filtre avant le worklet pour r√©duire la charge de traitement
      const lowpassFilter = this.audioContext.createBiquadFilter();
      lowpassFilter.type = 'lowpass';
      lowpassFilter.frequency.value = 3500; // Limite √† 3.5kHz (sous Nyquist 4kHz pour 8kHz)
      lowpassFilter.Q.value = 0.707; // Q optimal (Butterworth) pour transition douce sans r√©sonance

      // 5) Create script processor for raw audio recording (before worklet)
      const bufferSize = 4096;
      this.recorderScriptNode = this.audioContext.createScriptProcessor(bufferSize, 1, 1);
      
      // Flag to track if we're recording (utiliser une r√©f√©rence partag√©e)
      this.isRecording = true;
      
      this.recorderScriptNode.onaudioprocess = (e) => {
        if (!this.isRecording || !this.audioContext) return; // V√©rifier que audioContext existe et qu'on enregistre encore
        
        // NOTE: La sauvegarde automatique des fichiers audio a √©t√© d√©sactiv√©e
        // pour √©viter les interruptions et bruits dans le flux audio en temps r√©el.
        // Le ScriptProcessorNode reste actif uniquement pour maintenir le flux audio.
        
        // Ne plus accumuler les donn√©es dans rawAudioBuffer pour √©conomiser la m√©moire
        // et √©viter les interruptions de traitement
      };
      
      // 6) Load and create worklet for RTP encoding FIRST (before connecting)
      const workletUrl = new URL('../worklets/mic-processor.worklet.js', import.meta.url);
      await this.audioContext.audioWorklet.addModule(workletUrl);
      this.node = new AudioWorkletNode(this.audioContext, 'mic-processor', { numberOfInputs: 1, numberOfOutputs: 0 });
      
      // 7) Connect audio chain with noise reduction filter:
      //    OPTIMIZED: source ‚Üí lowpassFilter ‚Üí worklet (encodes RTP with noise reduction)
      //    PARALLEL: source ‚Üí recorder ‚Üí analyser (records audio without feedback)
      
      // Cr√©er un AnalyserNode qui ne produit pas de sortie audio mais maintient le ScriptProcessorNode actif
      // L'AnalyserNode permet au ScriptProcessorNode de fonctionner sans cr√©er de feedback
      const analyser = this.audioContext.createAnalyser();
      analyser.fftSize = 2048;
      
      // Cha√Æne principale avec filtre passe-bas pour r√©duire les bruits haute fr√©quence
      source.connect(lowpassFilter);
      lowpassFilter.connect(this.node);
      
      // Cha√Æne parall√®le pour l'enregistrement (sans filtre pour garder la qualit√© originale)
      source.connect(this.recorderScriptNode);
      // Connecter √† un AnalyserNode au lieu de la destination pour √©viter le feedback
      this.recorderScriptNode.connect(analyser);
      // L'AnalyserNode n'a pas besoin d'√™tre connect√© √† la destination
      
      // Store recording start time
      this.recordingStartTime = Date.now();
      
      // Nettoyer le buffer audio pour √©viter les donn√©es r√©siduelles
      // (la sauvegarde automatique est d√©sactiv√©e)
      this.rawAudioBuffer = [];
      this.recordingCounter = 0;

      // 4) Receive RTP packets from worklet and send over WS (RTP PCMU with headers)
      // SYST√àME DE BACKPRESSURE : Limiter le nombre de paquets en attente
      let chunkCount = 0;
      let pendingPackets = 0; // Compteur de paquets en attente d'envoi
      const MAX_PENDING_PACKETS = 10; // Maximum de paquets en attente
      
      this.node.port.onmessage = (ev: MessageEvent) => {
        // Arr√™ter si on n'enregistre plus
        if (!this.isRecording) return;
        
        const rtpPacket: Uint8Array = ev.data;
        if (!rtpPacket || !(rtpPacket instanceof Uint8Array)) return;
        
        chunkCount++;
        
        // BACKPRESSURE : Si trop de paquets en attente, drop ce paquet
        if (pendingPackets >= MAX_PENDING_PACKETS) {
          if (chunkCount % 100 === 0) {
            console.warn(`‚ö†Ô∏è Backpressure: dropping RTP packet #${chunkCount} (${pendingPackets} packets pending)`);
          }
          return; // Drop ce paquet pour √©viter la saturation
        }
        
        // Log moins fr√©quemment pour r√©duire le bruit
        if (chunkCount === 1 || chunkCount % 100 === 0) {
          console.log(`üì¶ RTP packet #${chunkCount}: ${rtpPacket.length} bytes`);
        }
        
        // V√©rifier que le WebSocket est ouvert avant d'envoyer
        if (!this.outboundWs || this.outboundWs.readyState !== WebSocket.OPEN) {
          if (chunkCount === 1) {
            console.warn(`‚ö†Ô∏è Outbound WebSocket not ready, stopping RTP packet sending. State: ${this.outboundWs?.readyState}`);
          }
          return;
        }
        
        // Encode RTP packet to base64
        const base64 = this.uint8ToBase64(rtpPacket);
        
        try {
          // Incr√©menter le compteur de paquets en attente
          pendingPackets++;
          
          this.outboundWs.send(JSON.stringify({ event: 'media', media: { payload: base64 } }));
          
          // D√©cr√©menter apr√®s l'envoi (simuler l'acknowledgment)
          // En r√©alit√©, on ne peut pas savoir quand le paquet est vraiment envoy√©,
          // donc on d√©cr√©mente apr√®s un court d√©lai
          setTimeout(() => {
            pendingPackets = Math.max(0, pendingPackets - 1);
          }, 20); // 20ms = temps approximatif d'envoi d'un paquet
          
          // Log moins fr√©quemment
          if (chunkCount === 1 || chunkCount % 100 === 0) {
            console.log(`‚úÖ Sent RTP packet #${chunkCount} via outbound WebSocket`);
          }
        } catch (error) {
          pendingPackets = Math.max(0, pendingPackets - 1);
          console.error(`‚ùå Error sending RTP packet #${chunkCount}:`, error);
        }
      };

      console.log('üéß Microphone capture started');
    } catch (err) {
      console.error('‚ùå Error starting microphone stream:', err);
      await this.stopCapture();
      throw err;
    }
  }

  private saveAudioAsMP3() {
    if (this.rawAudioBuffer.length === 0) return;

    try {
      console.log(`üíæ Preparing to save audio file (3 seconds, ${this.rawAudioBuffer.length} chunks)`);
      
      // Flatten the buffer to a single Float32Array
      const totalSamples = this.rawAudioBuffer.reduce((sum, arr) => sum + arr.length, 0);
      const audioData = new Float32Array(totalSamples);
      let offset = 0;
      for (const chunk of this.rawAudioBuffer) {
        audioData.set(chunk, offset);
        offset += chunk.length;
      }

      // Convert to WAV and download
      const wavBlob = this.float32ToWav(audioData, this.audioContext!.sampleRate);
      const timestamp = new Date().toISOString().replace(/[:.]/g, '-').split('T')[0];
      const time = new Date().toTimeString().replace(/[:.]/g, '-').split(' ')[0];
      const filename = `outbound-call-${timestamp}-${time}-part${this.recordingCounter}.wav`;
      
      const url = URL.createObjectURL(wavBlob);
      const a = document.createElement('a');
      a.href = url;
      a.download = filename;
      document.body.appendChild(a);
      a.click();
      document.body.removeChild(a);
      URL.revokeObjectURL(url);
      
      console.log(`‚úÖ Saved audio file: ${filename} (${audioData.length} samples, ${this.audioContext!.sampleRate}Hz)`);

      // Clear buffer for next 3 seconds and increment counter
      this.rawAudioBuffer = [];
      this.recordingCounter++;
    } catch (error) {
      console.error('‚ùå Error saving audio file:', error);
    }
  }

  private float32ToWav(pcmData: Float32Array, sampleRate: number): Blob {
    // Convert Float32 [-1, 1] to Int16 [-32768, 32767]
    const length = pcmData.length;
    const buffer = new ArrayBuffer(44 + length * 2);
    const view = new DataView(buffer);

    // WAV header helper function
    const writeString = (offset: number, string: string) => {
      for (let i = 0; i < string.length; i++) {
        view.setUint8(offset + i, string.charCodeAt(i));
      }
    };

    // Write WAV header
    writeString(0, 'RIFF');
    view.setUint32(4, 36 + length * 2, true); // File size - 8
    writeString(8, 'WAVE');
    writeString(12, 'fmt ');
    view.setUint32(16, 16, true); // Subchunk1Size
    view.setUint16(20, 1, true); // AudioFormat (1 = PCM)
    view.setUint16(22, 1, true); // NumChannels
    view.setUint32(24, sampleRate, true); // SampleRate
    view.setUint32(28, sampleRate * 2, true); // ByteRate
    view.setUint16(32, 2, true); // BlockAlign
    view.setUint16(34, 16, true); // BitsPerSample
    writeString(36, 'data');
    view.setUint32(40, length * 2, true); // Subchunk2Size

    // Write PCM data with Float32 to Int16 conversion
    for (let i = 0; i < length; i++) {
      let s = Math.max(-1, Math.min(1, pcmData[i]));
      const sample = s < 0 ? s * 0x8000 : s * 0x7FFF;
      view.setInt16(44 + i * 2, sample, true);
    }

    return new Blob([buffer], { type: 'audio/wav' });
  }

  async stopCapture() {
    console.log('‚èπÔ∏è Stopping microphone stream');
    this.hasStartedCapture = false; // R√©initialiser le flag
    // Arr√™ter l'enregistrement d'abord pour √©viter les callbacks apr√®s le cleanup
    this.isRecording = false;
    
    // NOTE: La sauvegarde automatique des fichiers audio a √©t√© d√©sactiv√©e
    // pour √©viter les interruptions et bruits dans le flux audio en temps r√©el.
    
    // Clear interval if set
    if (this.recordingInterval) {
      clearInterval(this.recordingInterval);
      this.recordingInterval = null;
    }
    
    // Arr√™ter le worklet d'abord pour √©viter l'envoi de paquets apr√®s la fermeture
    if (this.node) {
      try {
        this.node.port.onmessage = null; // Arr√™ter les callbacks
        this.node.disconnect();
      } catch (_) {}
    }
    
    // Arr√™ter le recorderScriptNode
    if (this.recorderScriptNode) {
      try {
        this.recorderScriptNode.onaudioprocess = null; // Arr√™ter les callbacks
        this.recorderScriptNode.disconnect();
      } catch (_) {}
    }
    
    // Arr√™ter le stream
    try { this.stream?.getTracks().forEach(t => t.stop()); } catch (_) {}
    
    // Fermer l'audioContext en dernier
    try { await this.audioContext?.close(); } catch (_) {}
    // We do NOT close the outbound WebSocket here; it's managed by the caller

    this.node = null;
    this.recorderScriptNode = null;
    this.stream = null;
    this.audioContext = null;
    this.rawAudioBuffer = [];
    this.recordingCounter = 0;
    // keep outboundWs reference (still owned by caller)
  }

  // Uint8Array -> base64
  private uint8ToBase64(u8: Uint8Array): string {
    let binary = '';
    const chunkSize = 0x8000;
    for (let i = 0; i < u8.length; i += chunkSize) {
      const chunk = u8.subarray(i, i + chunkSize);
      binary += String.fromCharCode.apply(null, chunk as unknown as number[]);
    }
    return btoa(binary);
  }
}
